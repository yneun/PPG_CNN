{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in ./.venv/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: absl-py in ./.venv/lib/python3.12/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in ./.venv/lib/python3.12/site-packages (from keras) (13.9.3)\n",
      "Requirement already satisfied: namex in ./.venv/lib/python3.12/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in ./.venv/lib/python3.12/site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in ./.venv/lib/python3.12/site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in ./.venv/lib/python3.12/site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.12/site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in ./.venv/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./.venv/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in ./.venv/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./.venv/lib/python3.12/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in ./.venv/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.9.3)\n",
      "Requirement already satisfied: namex in ./.venv/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in ./.venv/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install keras\n",
    "%pip install tensorflow\n",
    "%pip install numpy pandas matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 17:53:58.913874: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Data Load\n",
    "ppg_041 = pd.read_csv('../Data/ppg_features_041.csv')\n",
    "ppg_427 = pd.read_csv('../Data/ppg_features_427.csv')\n",
    "ppg_212 = pd.read_csv('../Data/ppg_features_212.csv')\n",
    "ppg_224 = pd.read_csv('../Data/ppg_features_224.csv')\n",
    "ppg_237 = pd.read_csv('../Data/ppg_features_237.csv')\n",
    "ppg_240 = pd.read_csv('../Data/ppg_features_240.csv')\n",
    "ppg_414 = pd.read_csv('../Data/ppg_features_414.csv')\n",
    "\n",
    "# SBP Data Load\n",
    "sbp_041 = pd.read_csv('../Data/sbp_041.csv')\n",
    "sbp_427 = pd.read_csv('../Data/sbp_427.csv')\n",
    "sbp_212 = pd.read_csv('../Data/sbp_212.csv')\n",
    "sbp_224 = pd.read_csv('../Data/sbp_224.csv')\n",
    "sbp_237 = pd.read_csv('../Data/sbp_237.csv')\n",
    "sbp_240 = pd.read_csv('../Data/sbp_240.csv')\n",
    "sbp_414 = pd.read_csv('../Data/sbp_414.csv')\n",
    "\n",
    "# DBP Data Load\n",
    "dbp_041 = pd.read_csv('../Data/dbp_041.csv')\n",
    "dbp_427 = pd.read_csv('../Data/dbp_427.csv')\n",
    "dbp_212 = pd.read_csv('../Data/dbp_212.csv')\n",
    "dbp_224 = pd.read_csv('../Data/dbp_224.csv')\n",
    "dbp_237 = pd.read_csv('../Data/dbp_237.csv')\n",
    "dbp_240 = pd.read_csv('../Data/dbp_240.csv')\n",
    "dbp_414 = pd.read_csv('../Data/dbp_414.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_combined shape: (5975, 26)\n",
      "X_test_combined shape: (1496, 26)\n",
      "SBP_Y_train_combined shape: (5975, 1)\n",
      "SBP_Y_test_combined shape: (1496, 1)\n",
      "DBP_Y_train_combined shape: (5975, 1)\n",
      "DBP_Y_test_combined shape: (1496, 1)\n"
     ]
    }
   ],
   "source": [
    "# List of datasets\n",
    "feature_data = [ppg_041,ppg_212,ppg_224,ppg_237,ppg_240,ppg_414,ppg_427]\n",
    "sbp_data = [sbp_041,sbp_212,sbp_224,sbp_237,sbp_240,sbp_414,sbp_427]\n",
    "dbp_data = [dbp_041,dbp_212,dbp_224,dbp_237,dbp_240,dbp_414,dbp_427]\n",
    "\n",
    "# List for saving training set and test set\n",
    "X_train_set = []\n",
    "X_test_set = []\n",
    "SBP_y_train_set =[]\n",
    "SBP_y_test_set = []\n",
    "DBP_y_train_set =[]\n",
    "DBP_y_test_set = []\n",
    "\n",
    "# For every datasets execute train_test_split\n",
    "for i in range(len(feature_data)):\n",
    "    X_train, X_test, SBP_Y_train, SBP_Y_test = train_test_split(feature_data[i], sbp_data[i], test_size=0.2, random_state=42)\n",
    "    _, _, DBP_Y_train, DBP_Y_test = train_test_split(feature_data[i], dbp_data[i], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Save training set and test set\n",
    "    X_train_set.append(X_train)\n",
    "    X_test_set.append(X_test)\n",
    "    SBP_y_train_set.append(SBP_Y_train)\n",
    "    SBP_y_test_set.append(SBP_Y_test)\n",
    "    DBP_y_train_set.append(DBP_Y_train)\n",
    "    DBP_y_test_set.append(DBP_Y_test)\n",
    "\n",
    "# Save the combined traning set and test set\n",
    "X_train_combined = pd.concat(X_train_set, axis=0)\n",
    "X_test_combined = pd.concat(X_test_set, axis=0)\n",
    "SBP_Y_train_combined = pd.concat(SBP_y_train_set, axis=0)\n",
    "SBP_Y_test_combined = pd.concat(SBP_y_test_set, axis=0)\n",
    "DBP_Y_train_combined = pd.concat(DBP_y_train_set, axis=0)\n",
    "DBP_Y_test_combined = pd.concat(DBP_y_test_set, axis=0)\n",
    "\n",
    "# Change the dimension\n",
    "# X_train_combined = X_train_combined.values.reshape(-1, X_train_combined.shape[1], 1)\n",
    "# X_test_combined = X_test_combined.values.reshape(-1, X_test_combined.shape[1], 1)\n",
    "# SBP_Y_train_combined = SBP_Y_train_combined.values.reshape(-1, SBP_Y_train_combined.shape[1],1)\n",
    "# SBP_Y_test_combined = SBP_Y_test_combined.values.reshape(-1, SBP_Y_test_combined.shape[1], 1)\n",
    "# DBP_Y_train_combined = DBP_Y_train_combined.values.reshape(-1, DBP_Y_train_combined[1], 1)\n",
    "# DBP_Y_test_combined = DBP_Y_test_combined.values.reshape(-1, DBP_Y_test_combined[1], 1)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"X_train_combined shape:\", X_train_combined.shape)\n",
    "print(\"X_test_combined shape:\", X_test_combined.shape)\n",
    "print(\"SBP_Y_train_combined shape:\", SBP_Y_train_combined.shape)\n",
    "print(\"SBP_Y_test_combined shape:\", SBP_Y_test_combined.shape)\n",
    "print(\"DBP_Y_train_combined shape:\", DBP_Y_train_combined.shape)\n",
    "print(\"DBP_Y_test_combined shape:\", DBP_Y_test_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = X_train_combined.values.reshape(-1, X_train_combined.shape[1], 1)\n",
    "X_test_combined = X_test_combined.values.reshape(-1, X_test_combined.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_combined shape: (5975, 26, 1)\n",
      "X_test_combined shape: (1496, 26, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_combined shape:\", X_train_combined.shape)\n",
    "print(\"X_test_combined shape:\", X_test_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sw/Documents/BCI_LAB/Implement/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 1D CNN 모델 정의 (회귀용)\n",
    "sbp_model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_combined.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1)  # 회귀 문제이므로 활성화 함수 없음\n",
    "])\n",
    "\n",
    "sbp_model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2104.9905 - mae: 33.4820 - val_loss: 421.6115 - val_mae: 14.9716\n",
      "Epoch 2/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 830.8054 - mae: 22.8023 - val_loss: 376.6068 - val_mae: 14.4486\n",
      "Epoch 3/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 728.2292 - mae: 21.2893 - val_loss: 319.6303 - val_mae: 13.3767\n",
      "Epoch 4/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 638.9374 - mae: 19.8762 - val_loss: 223.1750 - val_mae: 11.7511\n",
      "Epoch 5/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 567.4926 - mae: 18.6708 - val_loss: 204.1869 - val_mae: 10.9945\n",
      "Epoch 6/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 567.5546 - mae: 18.5246 - val_loss: 184.7756 - val_mae: 10.3708\n",
      "Epoch 7/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 502.6161 - mae: 17.6778 - val_loss: 192.5422 - val_mae: 10.5353\n",
      "Epoch 8/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 513.8581 - mae: 17.7220 - val_loss: 238.2017 - val_mae: 12.2224\n",
      "Epoch 9/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 528.0688 - mae: 17.8027 - val_loss: 167.4496 - val_mae: 9.8409\n",
      "Epoch 10/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 496.3274 - mae: 17.2682 - val_loss: 162.5165 - val_mae: 9.5940\n",
      "Epoch 11/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 506.3187 - mae: 17.5378 - val_loss: 237.3759 - val_mae: 12.3170\n",
      "Epoch 12/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 503.8836 - mae: 17.3868 - val_loss: 216.1475 - val_mae: 11.4876\n",
      "Epoch 13/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 457.9384 - mae: 16.6009 - val_loss: 167.5188 - val_mae: 9.9462\n",
      "Epoch 14/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 485.6792 - mae: 17.1199 - val_loss: 197.5787 - val_mae: 11.2014\n",
      "Epoch 15/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 492.7924 - mae: 17.3176 - val_loss: 158.6447 - val_mae: 9.5932\n",
      "Epoch 16/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 495.0341 - mae: 17.3051 - val_loss: 248.3442 - val_mae: 12.2465\n",
      "Epoch 17/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 507.7834 - mae: 17.6155 - val_loss: 152.1455 - val_mae: 9.3272\n",
      "Epoch 18/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 476.7815 - mae: 16.9278 - val_loss: 151.0079 - val_mae: 9.1881\n",
      "Epoch 19/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 466.6277 - mae: 16.7789 - val_loss: 185.8677 - val_mae: 10.4947\n",
      "Epoch 20/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 467.9219 - mae: 16.8989 - val_loss: 148.8813 - val_mae: 9.1819\n",
      "Epoch 21/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 485.0649 - mae: 16.8851 - val_loss: 197.4040 - val_mae: 10.6757\n",
      "Epoch 22/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 481.8922 - mae: 17.2754 - val_loss: 216.2543 - val_mae: 11.8200\n",
      "Epoch 23/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 450.4691 - mae: 16.6330 - val_loss: 186.5356 - val_mae: 10.6171\n",
      "Epoch 24/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 473.3649 - mae: 16.8938 - val_loss: 152.5432 - val_mae: 9.3755\n",
      "Epoch 25/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 427.0803 - mae: 16.0594 - val_loss: 142.4300 - val_mae: 8.9130\n",
      "Epoch 26/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 411.4778 - mae: 15.8114 - val_loss: 165.4304 - val_mae: 9.9605\n",
      "Epoch 27/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 429.4149 - mae: 16.0023 - val_loss: 165.4714 - val_mae: 9.9043\n",
      "Epoch 28/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 459.5363 - mae: 16.6941 - val_loss: 156.5498 - val_mae: 9.4368\n",
      "Epoch 29/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 428.2080 - mae: 16.1617 - val_loss: 145.9937 - val_mae: 9.1229\n",
      "Epoch 30/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 440.6379 - mae: 16.3992 - val_loss: 141.4551 - val_mae: 8.9744\n",
      "Epoch 31/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 447.0696 - mae: 16.4466 - val_loss: 155.7695 - val_mae: 9.2407\n",
      "Epoch 32/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 415.1092 - mae: 15.9464 - val_loss: 154.9989 - val_mae: 9.5943\n",
      "Epoch 33/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 438.9173 - mae: 16.3368 - val_loss: 174.7663 - val_mae: 10.4299\n",
      "Epoch 34/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 429.7130 - mae: 16.0598 - val_loss: 192.5490 - val_mae: 10.9786\n",
      "Epoch 35/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 446.5020 - mae: 16.2352 - val_loss: 157.7630 - val_mae: 9.6117\n",
      "Epoch 36/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 460.1461 - mae: 16.6566 - val_loss: 140.4250 - val_mae: 8.7508\n",
      "Epoch 37/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 456.7070 - mae: 16.5638 - val_loss: 194.0649 - val_mae: 10.7141\n",
      "Epoch 38/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 434.5359 - mae: 16.1163 - val_loss: 147.2303 - val_mae: 9.0794\n",
      "Epoch 39/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 457.3554 - mae: 16.2625 - val_loss: 140.8758 - val_mae: 8.8870\n",
      "Epoch 40/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 435.2252 - mae: 16.0710 - val_loss: 137.8189 - val_mae: 8.7806\n",
      "Epoch 41/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 435.9494 - mae: 16.0481 - val_loss: 137.4826 - val_mae: 8.6932\n",
      "Epoch 42/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 437.1668 - mae: 15.9725 - val_loss: 151.0984 - val_mae: 9.3533\n",
      "Epoch 43/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 443.3349 - mae: 16.1727 - val_loss: 141.7786 - val_mae: 8.9544\n",
      "Epoch 44/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 420.7513 - mae: 15.9512 - val_loss: 136.3712 - val_mae: 8.6780\n",
      "Epoch 45/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 407.2543 - mae: 15.6509 - val_loss: 141.8332 - val_mae: 9.0116\n",
      "Epoch 46/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 421.9691 - mae: 16.0170 - val_loss: 150.6622 - val_mae: 9.2362\n",
      "Epoch 47/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 420.6529 - mae: 15.8108 - val_loss: 162.0163 - val_mae: 9.8679\n",
      "Epoch 48/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 431.4178 - mae: 16.0864 - val_loss: 164.8795 - val_mae: 9.8842\n",
      "Epoch 49/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 436.3598 - mae: 16.1478 - val_loss: 144.6686 - val_mae: 9.0734\n",
      "Epoch 50/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 412.1915 - mae: 15.6280 - val_loss: 148.7378 - val_mae: 9.2959\n",
      "Early stopping did not occur\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 133.8752 - mae: 8.3368\n",
      "Validation MAE: 8.68\n",
      "Validation Loss: 136.371201\n"
     ]
    }
   ],
   "source": [
    "# 콜백 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# 모델 학습\n",
    "history = sbp_model.fit(\n",
    "    X_train_combined, SBP_Y_train_combined,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_combined, SBP_Y_test_combined),\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "if early_stopping.stopped_epoch > 0:\n",
    "    print(f\"Early stopping occurred at epoch {early_stopping.stopped_epoch}\")\n",
    "else:\n",
    "    print(\"Early stopping did not occur\")\n",
    "\n",
    "\n",
    "# 검증 데이터로 모델 평가\n",
    "val_loss, val_mae = sbp_model.evaluate(X_test_combined, SBP_Y_test_combined)\n",
    "print(f\"Validation MAE: {val_mae:.2f}\")\n",
    "print(f\"Validation Loss: {val_loss:2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 420.6417 - mae: 16.0283 - val_loss: 177.4781 - val_mae: 10.4273\n",
      "Epoch 2/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 448.0482 - mae: 16.3584 - val_loss: 147.8187 - val_mae: 9.2814\n",
      "Epoch 3/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 428.7095 - mae: 16.0417 - val_loss: 174.5527 - val_mae: 10.3866\n",
      "Epoch 4/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 412.9308 - mae: 15.7554 - val_loss: 139.8834 - val_mae: 8.8873\n",
      "Epoch 5/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 425.8672 - mae: 16.1773 - val_loss: 137.4271 - val_mae: 8.7349\n",
      "Epoch 6/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 431.3186 - mae: 16.2000 - val_loss: 133.3212 - val_mae: 8.4739\n",
      "Epoch 7/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 410.3465 - mae: 15.7681 - val_loss: 136.8907 - val_mae: 8.7108\n",
      "Epoch 8/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 437.0156 - mae: 16.1256 - val_loss: 144.7146 - val_mae: 9.1630\n",
      "Epoch 9/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 405.5882 - mae: 15.5806 - val_loss: 199.3355 - val_mae: 11.0711\n",
      "Epoch 10/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 420.0209 - mae: 15.9548 - val_loss: 134.4318 - val_mae: 8.5989\n",
      "Epoch 11/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 430.0125 - mae: 16.0963 - val_loss: 151.3100 - val_mae: 9.1022\n",
      "Epoch 12/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 418.8489 - mae: 15.8902 - val_loss: 172.5434 - val_mae: 9.8817\n",
      "Epoch 13/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 402.3647 - mae: 15.5956 - val_loss: 162.9241 - val_mae: 9.8482\n",
      "Epoch 14/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 415.3240 - mae: 15.7165 - val_loss: 181.4200 - val_mae: 10.4624\n",
      "Epoch 15/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 422.0166 - mae: 15.9911 - val_loss: 156.2399 - val_mae: 9.5276\n",
      "Epoch 16/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 395.9710 - mae: 15.3665 - val_loss: 174.7875 - val_mae: 10.4408\n",
      "Epoch 17/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 402.3894 - mae: 15.6714 - val_loss: 138.8779 - val_mae: 8.8576\n",
      "Epoch 18/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 400.4332 - mae: 15.3384 - val_loss: 146.6763 - val_mae: 9.1261\n",
      "Epoch 19/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 413.2237 - mae: 15.6967 - val_loss: 139.0268 - val_mae: 8.5824\n",
      "Epoch 20/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 404.5381 - mae: 15.6025 - val_loss: 132.6234 - val_mae: 8.5040\n",
      "Epoch 21/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 386.7020 - mae: 15.1548 - val_loss: 136.2566 - val_mae: 8.5745\n",
      "Epoch 22/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 405.2410 - mae: 15.6994 - val_loss: 170.9058 - val_mae: 10.1766\n",
      "Epoch 23/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 411.4102 - mae: 15.5881 - val_loss: 136.4938 - val_mae: 8.6886\n",
      "Epoch 24/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 404.6970 - mae: 15.4750 - val_loss: 134.0348 - val_mae: 8.6857\n",
      "Epoch 25/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 404.1445 - mae: 15.3982 - val_loss: 152.3576 - val_mae: 9.3806\n",
      "Epoch 26/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 402.5959 - mae: 15.4578 - val_loss: 142.6489 - val_mae: 8.8975\n",
      "Epoch 27/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 420.2373 - mae: 15.7954 - val_loss: 132.3780 - val_mae: 8.5559\n",
      "Epoch 28/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 379.4474 - mae: 15.0852 - val_loss: 141.0860 - val_mae: 8.8190\n",
      "Epoch 29/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 398.3164 - mae: 15.7031 - val_loss: 134.0522 - val_mae: 8.5613\n",
      "Epoch 30/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 419.3604 - mae: 15.7418 - val_loss: 158.6283 - val_mae: 9.6909\n",
      "Epoch 31/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 413.1174 - mae: 15.6283 - val_loss: 155.7530 - val_mae: 9.3419\n",
      "Epoch 32/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 412.6372 - mae: 15.8509 - val_loss: 133.7775 - val_mae: 8.5342\n",
      "Epoch 33/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 402.4701 - mae: 15.5139 - val_loss: 141.5233 - val_mae: 8.8589\n",
      "Epoch 34/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 374.4453 - mae: 15.1737 - val_loss: 139.6706 - val_mae: 8.9648\n",
      "Epoch 35/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 403.9696 - mae: 15.6666 - val_loss: 131.8738 - val_mae: 8.5829\n",
      "Epoch 36/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 398.3390 - mae: 15.4783 - val_loss: 132.0664 - val_mae: 8.5023\n",
      "Epoch 37/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 390.6000 - mae: 15.3608 - val_loss: 152.6827 - val_mae: 9.0749\n",
      "Epoch 38/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 395.1423 - mae: 15.4168 - val_loss: 131.6700 - val_mae: 8.4208\n",
      "Epoch 39/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 415.1727 - mae: 15.7736 - val_loss: 150.5192 - val_mae: 9.4027\n",
      "Epoch 40/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 393.0885 - mae: 15.3347 - val_loss: 136.1021 - val_mae: 8.5603\n",
      "Epoch 41/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 373.3242 - mae: 15.1449 - val_loss: 166.5013 - val_mae: 10.0865\n",
      "Epoch 42/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 389.9746 - mae: 15.2100 - val_loss: 140.5781 - val_mae: 8.7605\n",
      "Epoch 43/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 386.5366 - mae: 15.2975 - val_loss: 147.8485 - val_mae: 9.3755\n",
      "Epoch 44/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 402.4406 - mae: 15.5301 - val_loss: 149.9113 - val_mae: 9.3870\n",
      "Epoch 45/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 376.4176 - mae: 15.1596 - val_loss: 144.0959 - val_mae: 9.1637\n",
      "Epoch 46/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 362.0596 - mae: 14.7868 - val_loss: 132.1171 - val_mae: 8.5585\n",
      "Epoch 47/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 374.0604 - mae: 14.9823 - val_loss: 144.0407 - val_mae: 9.0193\n",
      "Epoch 48/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 395.2436 - mae: 15.5037 - val_loss: 137.3869 - val_mae: 8.7783\n",
      "Epoch 49/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 381.0962 - mae: 15.2448 - val_loss: 134.2686 - val_mae: 8.5710\n",
      "Epoch 50/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 375.2481 - mae: 15.0535 - val_loss: 139.8439 - val_mae: 8.8475\n",
      "Early stopping did not occur\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 128.6185 - mae: 7.9738\n",
      "Validation MAE: 8.42\n",
      "Validation Loss: 131.670013\n"
     ]
    }
   ],
   "source": [
    "# 콜백 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# 모델 학습\n",
    "history = sbp_model.fit(\n",
    "    X_train_combined, SBP_Y_train_combined,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_combined, SBP_Y_test_combined),\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "if early_stopping.stopped_epoch > 0:\n",
    "    print(f\"Early stopping occurred at epoch {early_stopping.stopped_epoch}\")\n",
    "else:\n",
    "    print(\"Early stopping did not occur\")\n",
    "\n",
    "\n",
    "# 검증 데이터로 모델 평가\n",
    "val_loss, val_mae = sbp_model.evaluate(X_test_combined, SBP_Y_test_combined)\n",
    "print(f\"Validation MAE: {val_mae:.2f}\")\n",
    "print(f\"Validation Loss: {val_loss:2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 375.2697 - mae: 15.1018 - val_loss: 148.1275 - val_mae: 8.9879\n",
      "Epoch 2/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 388.9445 - mae: 15.4271 - val_loss: 136.3534 - val_mae: 8.7761\n",
      "Epoch 3/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 365.0065 - mae: 14.9590 - val_loss: 129.3083 - val_mae: 8.3473\n",
      "Epoch 4/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 410.2056 - mae: 15.8127 - val_loss: 154.3818 - val_mae: 9.6530\n",
      "Epoch 5/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 388.4202 - mae: 15.1773 - val_loss: 133.4867 - val_mae: 8.5087\n",
      "Epoch 6/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 400.5582 - mae: 15.4752 - val_loss: 136.1529 - val_mae: 8.5696\n",
      "Epoch 7/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 396.9964 - mae: 15.4121 - val_loss: 167.8117 - val_mae: 10.0972\n",
      "Epoch 8/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 381.7303 - mae: 15.0798 - val_loss: 137.1218 - val_mae: 8.8575\n",
      "Epoch 9/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 382.7870 - mae: 15.0685 - val_loss: 131.5567 - val_mae: 8.5683\n",
      "Epoch 10/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 377.0616 - mae: 14.8393 - val_loss: 133.8805 - val_mae: 8.6977\n",
      "Epoch 11/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 404.2360 - mae: 15.3983 - val_loss: 147.6633 - val_mae: 9.2736\n",
      "Epoch 12/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 397.0933 - mae: 15.3484 - val_loss: 136.1301 - val_mae: 8.7870\n",
      "Epoch 13/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 362.5267 - mae: 14.7333 - val_loss: 128.8730 - val_mae: 8.2871\n",
      "Epoch 14/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 399.0545 - mae: 15.4373 - val_loss: 130.6999 - val_mae: 8.4832\n",
      "Epoch 15/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 367.1317 - mae: 14.7666 - val_loss: 142.6006 - val_mae: 9.0236\n",
      "Epoch 16/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 381.4187 - mae: 15.0167 - val_loss: 127.1293 - val_mae: 8.2869\n",
      "Epoch 17/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 372.4884 - mae: 14.9146 - val_loss: 149.1427 - val_mae: 8.9510\n",
      "Epoch 18/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 386.9416 - mae: 15.2762 - val_loss: 129.3470 - val_mae: 8.4283\n",
      "Epoch 19/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 373.0440 - mae: 14.9468 - val_loss: 145.5780 - val_mae: 9.1821\n",
      "Epoch 20/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 393.7420 - mae: 15.2804 - val_loss: 129.9783 - val_mae: 8.3544\n",
      "Epoch 21/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 370.7627 - mae: 14.8335 - val_loss: 141.4633 - val_mae: 8.7737\n",
      "Epoch 22/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 359.1415 - mae: 14.7743 - val_loss: 137.7932 - val_mae: 8.8419\n",
      "Epoch 23/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 362.7867 - mae: 14.8685 - val_loss: 186.4564 - val_mae: 10.9445\n",
      "Epoch 24/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 389.7826 - mae: 15.2827 - val_loss: 199.8418 - val_mae: 11.2304\n",
      "Epoch 25/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 373.7872 - mae: 15.1233 - val_loss: 133.8354 - val_mae: 8.5814\n",
      "Epoch 26/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 365.9702 - mae: 14.8509 - val_loss: 141.6703 - val_mae: 8.9909\n",
      "Epoch 27/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 358.7267 - mae: 14.7527 - val_loss: 147.8306 - val_mae: 9.3387\n",
      "Epoch 28/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 377.0854 - mae: 15.2575 - val_loss: 130.9701 - val_mae: 8.5400\n",
      "Epoch 29/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 370.3387 - mae: 14.9451 - val_loss: 136.8805 - val_mae: 8.7271\n",
      "Epoch 30/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 361.7424 - mae: 14.8304 - val_loss: 137.2071 - val_mae: 8.7871\n",
      "Epoch 31/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 362.7203 - mae: 14.8378 - val_loss: 148.6343 - val_mae: 9.4160\n",
      "Epoch 32/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 365.5223 - mae: 14.7358 - val_loss: 137.7061 - val_mae: 8.7574\n",
      "Epoch 33/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 387.5837 - mae: 15.3069 - val_loss: 168.2608 - val_mae: 10.1827\n",
      "Epoch 34/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 370.9108 - mae: 15.0003 - val_loss: 137.9128 - val_mae: 8.7246\n",
      "Epoch 35/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 372.1125 - mae: 14.9149 - val_loss: 129.2852 - val_mae: 8.4163\n",
      "Epoch 36/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 381.9720 - mae: 15.0244 - val_loss: 134.6494 - val_mae: 8.5470\n",
      "Early stopping occurred at epoch 35\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 122.4358 - mae: 7.7258\n",
      "Validation MAE: 8.29\n",
      "Validation Loss: 127.129326\n"
     ]
    }
   ],
   "source": [
    "# 콜백 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# 모델 학습\n",
    "history = sbp_model.fit(\n",
    "    X_train_combined, SBP_Y_train_combined,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_combined, SBP_Y_test_combined),\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "if early_stopping.stopped_epoch > 0:\n",
    "    print(f\"Early stopping occurred at epoch {early_stopping.stopped_epoch}\")\n",
    "else:\n",
    "    print(\"Early stopping did not occur\")\n",
    "\n",
    "\n",
    "# 검증 데이터로 모델 평가\n",
    "val_loss, val_mae = sbp_model.evaluate(X_test_combined, SBP_Y_test_combined)\n",
    "print(f\"Validation MAE: {val_mae:.2f}\")\n",
    "print(f\"Validation Loss: {val_loss:2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 372.4453 - mae: 15.0384 - val_loss: 137.7368 - val_mae: 8.8599\n",
      "Epoch 2/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 372.3409 - mae: 14.8139 - val_loss: 134.4173 - val_mae: 8.6685\n",
      "Epoch 3/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 378.6031 - mae: 14.9039 - val_loss: 132.6905 - val_mae: 8.4663\n",
      "Epoch 4/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 364.5589 - mae: 14.7012 - val_loss: 131.3856 - val_mae: 8.5496\n",
      "Epoch 5/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 390.2239 - mae: 15.1259 - val_loss: 128.2537 - val_mae: 8.3537\n",
      "Epoch 6/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 372.0381 - mae: 14.7341 - val_loss: 170.7204 - val_mae: 10.3042\n",
      "Epoch 7/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 379.2394 - mae: 14.8785 - val_loss: 126.9931 - val_mae: 8.2530\n",
      "Epoch 8/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 374.4845 - mae: 14.9952 - val_loss: 127.3723 - val_mae: 8.3235\n",
      "Epoch 9/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 386.4741 - mae: 15.2008 - val_loss: 152.2348 - val_mae: 9.4240\n",
      "Epoch 10/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 367.7067 - mae: 14.6881 - val_loss: 131.0412 - val_mae: 8.5526\n",
      "Epoch 11/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 367.8238 - mae: 14.4400 - val_loss: 147.1548 - val_mae: 9.2356\n",
      "Epoch 12/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 369.5861 - mae: 14.8081 - val_loss: 136.9656 - val_mae: 8.6539\n",
      "Epoch 13/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 382.7644 - mae: 15.0827 - val_loss: 127.0251 - val_mae: 8.2376\n",
      "Epoch 14/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 366.9369 - mae: 14.8759 - val_loss: 127.4325 - val_mae: 8.2440\n",
      "Epoch 15/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 374.7070 - mae: 15.0775 - val_loss: 131.6368 - val_mae: 8.5567\n",
      "Epoch 16/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 362.5951 - mae: 14.5624 - val_loss: 133.6071 - val_mae: 8.6654\n",
      "Epoch 17/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 352.9609 - mae: 14.4058 - val_loss: 128.3647 - val_mae: 8.3555\n",
      "Epoch 18/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 372.7665 - mae: 15.0110 - val_loss: 127.1598 - val_mae: 8.2482\n",
      "Epoch 19/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 352.9390 - mae: 14.6117 - val_loss: 171.5865 - val_mae: 10.2266\n",
      "Epoch 20/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 358.8001 - mae: 14.7138 - val_loss: 130.7965 - val_mae: 8.5010\n",
      "Epoch 21/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 370.6548 - mae: 14.8118 - val_loss: 149.5840 - val_mae: 9.3187\n",
      "Epoch 22/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 367.2108 - mae: 14.8405 - val_loss: 145.6033 - val_mae: 9.2975\n",
      "Epoch 23/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 375.8324 - mae: 14.8172 - val_loss: 130.4236 - val_mae: 8.4750\n",
      "Epoch 24/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 359.8756 - mae: 14.6493 - val_loss: 137.1494 - val_mae: 8.7831\n",
      "Epoch 25/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 383.7659 - mae: 15.2461 - val_loss: 127.3276 - val_mae: 8.3067\n",
      "Epoch 26/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 356.0562 - mae: 14.5913 - val_loss: 128.2826 - val_mae: 8.3638\n",
      "Epoch 27/60\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 377.6664 - mae: 15.1024 - val_loss: 131.2722 - val_mae: 8.3750\n",
      "Early stopping occurred at epoch 26\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 123.9919 - mae: 7.8108\n",
      "Validation MAE: 8.25\n",
      "Validation Loss: 126.993126\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "history = sbp_model.fit(\n",
    "    X_train_combined, SBP_Y_train_combined,\n",
    "    epochs=60,\n",
    "    batch_size=50,\n",
    "    validation_data=(X_test_combined, SBP_Y_test_combined),\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "if early_stopping.stopped_epoch > 0:\n",
    "    print(f\"Early stopping occurred at epoch {early_stopping.stopped_epoch}\")\n",
    "else:\n",
    "    print(\"Early stopping did not occur\")\n",
    "\n",
    "# 검증 데이터로 모델 평가\n",
    "val_loss, val_mae = sbp_model.evaluate(X_test_combined, SBP_Y_test_combined)\n",
    "print(f\"Validation MAE: {val_mae:.2f}\")\n",
    "print(f\"Validation Loss: {val_loss:2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 369.1114 - mae: 14.8585 - val_loss: 129.6820 - val_mae: 8.3216\n",
      "Epoch 2/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 361.4060 - mae: 14.8227 - val_loss: 133.4282 - val_mae: 8.5089\n",
      "Epoch 3/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 361.2442 - mae: 14.6649 - val_loss: 138.7159 - val_mae: 8.7961\n",
      "Epoch 4/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 372.9525 - mae: 14.8950 - val_loss: 128.7979 - val_mae: 8.3958\n",
      "Epoch 5/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 372.3510 - mae: 14.8753 - val_loss: 143.8685 - val_mae: 9.1444\n",
      "Epoch 6/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 377.1153 - mae: 14.9644 - val_loss: 131.6835 - val_mae: 8.5489\n",
      "Epoch 7/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 373.3239 - mae: 14.9451 - val_loss: 153.6161 - val_mae: 9.5584\n",
      "Epoch 8/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 387.3122 - mae: 14.9737 - val_loss: 130.6747 - val_mae: 8.5363\n",
      "Epoch 9/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 371.4352 - mae: 14.9233 - val_loss: 128.3416 - val_mae: 8.3332\n",
      "Epoch 10/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 368.2831 - mae: 14.9248 - val_loss: 126.9106 - val_mae: 8.2648\n",
      "Epoch 11/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 362.6195 - mae: 14.6993 - val_loss: 124.8892 - val_mae: 8.1626\n",
      "Epoch 12/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 383.7582 - mae: 15.0218 - val_loss: 131.8057 - val_mae: 8.4047\n",
      "Epoch 13/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 357.6461 - mae: 14.6775 - val_loss: 142.1896 - val_mae: 9.0325\n",
      "Epoch 14/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 349.5848 - mae: 14.3889 - val_loss: 166.2382 - val_mae: 10.0882\n",
      "Epoch 15/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 369.4814 - mae: 14.8584 - val_loss: 129.2073 - val_mae: 8.4522\n",
      "Epoch 16/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 369.5551 - mae: 14.7313 - val_loss: 127.0990 - val_mae: 8.3056\n",
      "Epoch 17/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 358.9120 - mae: 14.6337 - val_loss: 131.5764 - val_mae: 8.5722\n",
      "Epoch 18/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 370.3300 - mae: 14.9041 - val_loss: 144.3955 - val_mae: 9.1769\n",
      "Epoch 19/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 373.8926 - mae: 15.0909 - val_loss: 129.6503 - val_mae: 8.3596\n",
      "Epoch 20/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 368.4063 - mae: 14.9243 - val_loss: 151.2535 - val_mae: 9.4444\n",
      "Epoch 21/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 373.8784 - mae: 14.9053 - val_loss: 137.9541 - val_mae: 8.7835\n",
      "Epoch 22/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 363.2877 - mae: 14.7592 - val_loss: 134.8917 - val_mae: 8.7773\n",
      "Epoch 23/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 352.5626 - mae: 14.5535 - val_loss: 127.5792 - val_mae: 8.4206\n",
      "Epoch 24/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 382.1602 - mae: 14.9815 - val_loss: 126.0237 - val_mae: 8.2782\n",
      "Epoch 25/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 370.0052 - mae: 14.6345 - val_loss: 132.8106 - val_mae: 8.6098\n",
      "Epoch 26/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 356.9018 - mae: 14.5830 - val_loss: 126.6608 - val_mae: 8.2968\n",
      "Epoch 27/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 374.6857 - mae: 14.9757 - val_loss: 125.5901 - val_mae: 8.2342\n",
      "Epoch 28/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 348.5410 - mae: 14.3938 - val_loss: 136.5625 - val_mae: 8.9025\n",
      "Epoch 29/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 382.4880 - mae: 15.1520 - val_loss: 140.9492 - val_mae: 8.8110\n",
      "Epoch 30/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 359.5416 - mae: 14.5181 - val_loss: 139.9533 - val_mae: 8.9124\n",
      "Epoch 31/500\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 342.4988 - mae: 14.4082 - val_loss: 136.2681 - val_mae: 8.7810\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 119.1350 - mae: 7.6489\n",
      "Validation MAE: 8.16\n",
      "Validation Loss: 124.889214\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "history = sbp_model.fit(\n",
    "    X_train_combined, SBP_Y_train_combined,\n",
    "    epochs=500,\n",
    "    batch_size=60,\n",
    "    validation_data=(X_test_combined, SBP_Y_test_combined),\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# 검증 데이터로 모델 평가\n",
    "val_loss, val_mae = sbp_model.evaluate(X_test_combined, SBP_Y_test_combined)\n",
    "print(f\"Validation MAE: {val_mae:.2f}\")\n",
    "print(f\"Validation Loss: {val_loss:2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sw/Documents/BCI_LAB/Implement/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 1D CNN 모델 정의 (회귀용)\n",
    "dbp_model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_combined.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1)  # 회귀 문제이므로 활성화 함수 없음\n",
    "])\n",
    "\n",
    "dbp_model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 814.0879 - mae: 19.6473 - val_loss: 153.7584 - val_mae: 9.7467\n",
      "Epoch 2/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 245.4563 - mae: 12.3811 - val_loss: 120.1664 - val_mae: 8.5868\n",
      "Epoch 3/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 206.5826 - mae: 11.2488 - val_loss: 96.3324 - val_mae: 7.8325\n",
      "Epoch 4/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 191.2813 - mae: 10.8097 - val_loss: 84.8620 - val_mae: 6.6026\n",
      "Epoch 5/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 165.2897 - mae: 10.0499 - val_loss: 81.4299 - val_mae: 7.0315\n",
      "Epoch 6/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 149.2821 - mae: 9.5124 - val_loss: 70.5648 - val_mae: 6.2014\n",
      "Epoch 7/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 150.9942 - mae: 9.4807 - val_loss: 71.7598 - val_mae: 6.0822\n",
      "Epoch 8/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 148.1656 - mae: 9.3043 - val_loss: 80.1840 - val_mae: 6.4716\n",
      "Epoch 9/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 150.8571 - mae: 9.3911 - val_loss: 65.3535 - val_mae: 5.7574\n",
      "Epoch 10/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 151.9072 - mae: 9.5458 - val_loss: 73.8991 - val_mae: 6.2563\n",
      "Epoch 11/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 148.0298 - mae: 9.2907 - val_loss: 69.0638 - val_mae: 6.0219\n",
      "Epoch 12/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 144.2652 - mae: 9.1674 - val_loss: 64.0341 - val_mae: 5.8010\n",
      "Epoch 13/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 143.9427 - mae: 9.2371 - val_loss: 67.6331 - val_mae: 5.8755\n",
      "Epoch 14/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 148.4434 - mae: 9.3097 - val_loss: 94.1351 - val_mae: 7.2218\n",
      "Epoch 15/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 153.1824 - mae: 9.4079 - val_loss: 59.5989 - val_mae: 5.5567\n",
      "Epoch 16/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 136.6985 - mae: 9.0796 - val_loss: 65.0979 - val_mae: 5.8170\n",
      "Epoch 17/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 142.8897 - mae: 9.1327 - val_loss: 61.6304 - val_mae: 5.5861\n",
      "Epoch 18/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 142.6710 - mae: 9.1367 - val_loss: 77.4380 - val_mae: 6.4970\n",
      "Epoch 19/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 135.9853 - mae: 8.9847 - val_loss: 62.6297 - val_mae: 5.7627\n",
      "Epoch 20/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 141.6858 - mae: 9.0664 - val_loss: 80.0185 - val_mae: 6.5630\n",
      "Early stopping occurred at epoch 19\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 59.2431 - mae: 5.5049\n",
      "Validation MAE: 5.56\n",
      "Validation Loss: 59.598927\n"
     ]
    }
   ],
   "source": [
    "# 콜백 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# 모델 학습\n",
    "history = dbp_model.fit(\n",
    "    X_train_combined, DBP_Y_train_combined,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_combined, DBP_Y_test_combined),\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "if early_stopping.stopped_epoch > 0:\n",
    "    print(f\"Early stopping occurred at epoch {early_stopping.stopped_epoch}\")\n",
    "else:\n",
    "    print(\"Early stopping did not occur\")\n",
    "\n",
    "\n",
    "# 검증 데이터로 모델 평가\n",
    "val_loss, val_mae = dbp_model.evaluate(X_test_combined, DBP_Y_test_combined)\n",
    "print(f\"Validation MAE: {val_mae:.2f}\")\n",
    "print(f\"Validation Loss: {val_loss:2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 138.5911 - mae: 8.9324 - val_loss: 61.7237 - val_mae: 5.6839\n",
      "Epoch 2/600\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 140.1083 - mae: 9.0309 - val_loss: 82.2216 - val_mae: 6.7296\n",
      "Epoch 3/600\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 136.7899 - mae: 8.9859 - val_loss: 67.4617 - val_mae: 5.8987\n",
      "Epoch 4/600\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 130.2689 - mae: 8.7558 - val_loss: 64.1510 - val_mae: 5.7665\n",
      "Epoch 5/600\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 132.9539 - mae: 8.7568 - val_loss: 69.0053 - val_mae: 6.0186\n",
      "Epoch 6/600\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 128.4164 - mae: 8.6921 - val_loss: 62.8696 - val_mae: 5.6856\n",
      "Early stopping occurred at epoch 5\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 59.1047 - mae: 5.5880\n",
      "Validation MAE: 5.68\n",
      "Validation Loss: 61.723732\n"
     ]
    }
   ],
   "source": [
    "# 콜백 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# 모델 학습\n",
    "history = dbp_model.fit(\n",
    "    X_train_combined, DBP_Y_train_combined,\n",
    "    epochs=600,\n",
    "    batch_size=50,\n",
    "    validation_data=(X_test_combined, DBP_Y_test_combined),\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "if early_stopping.stopped_epoch > 0:\n",
    "    print(f\"Early stopping occurred at epoch {early_stopping.stopped_epoch}\")\n",
    "else:\n",
    "    print(\"Early stopping did not occur\")\n",
    "\n",
    "\n",
    "# 검증 데이터로 모델 평가\n",
    "val_loss, val_mae = dbp_model.evaluate(X_test_combined, DBP_Y_test_combined)\n",
    "print(f\"Validation MAE: {val_mae:.2f}\")\n",
    "print(f\"Validation Loss: {val_loss:2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 133.4226 - mae: 8.8193 - val_loss: 73.7024 - val_mae: 6.2942\n",
      "Epoch 2/300\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 136.5176 - mae: 8.9118 - val_loss: 60.4125 - val_mae: 5.5540\n",
      "Epoch 3/300\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 137.4954 - mae: 8.9707 - val_loss: 61.1457 - val_mae: 5.5915\n",
      "Epoch 4/300\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 136.0044 - mae: 8.8948 - val_loss: 60.2669 - val_mae: 5.6022\n",
      "Epoch 5/300\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 133.8607 - mae: 8.8109 - val_loss: 63.2098 - val_mae: 5.7358\n",
      "Epoch 6/300\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 137.8876 - mae: 9.0093 - val_loss: 59.4788 - val_mae: 5.5395\n",
      "Epoch 7/300\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 140.1936 - mae: 9.0099 - val_loss: 65.3370 - val_mae: 5.8065\n",
      "Epoch 8/300\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 131.9721 - mae: 8.8464 - val_loss: 59.1911 - val_mae: 5.5990\n",
      "Epoch 9/300\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 123.5879 - mae: 8.5475 - val_loss: 62.5151 - val_mae: 5.9609\n",
      "Epoch 10/300\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 144.3881 - mae: 9.1537 - val_loss: 61.4887 - val_mae: 5.6491\n",
      "Epoch 11/300\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 141.5059 - mae: 9.0084 - val_loss: 59.8859 - val_mae: 5.6551\n",
      "Epoch 12/300\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 140.7225 - mae: 9.0818 - val_loss: 64.7179 - val_mae: 5.8995\n",
      "Epoch 13/300\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 131.2724 - mae: 8.8583 - val_loss: 61.4049 - val_mae: 5.6377\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 56.4071 - mae: 5.5352\n",
      "Validation MAE: 5.60\n",
      "Validation Loss: 59.191071\n"
     ]
    }
   ],
   "source": [
    "# 콜백 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# 모델 학습\n",
    "history = dbp_model.fit(\n",
    "    X_train_combined, DBP_Y_train_combined,\n",
    "    epochs=300,\n",
    "    batch_size=60,\n",
    "    validation_data=(X_test_combined, DBP_Y_test_combined),\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# 검증 데이터로 모델 평가\n",
    "val_loss, val_mae = dbp_model.evaluate(X_test_combined, DBP_Y_test_combined)\n",
    "print(f\"Validation MAE: {val_mae:.2f}\")\n",
    "print(f\"Validation Loss: {val_loss:2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 140.1711 - mae: 8.9223 - val_loss: 66.7969 - val_mae: 5.8836\n",
      "Epoch 2/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 137.4087 - mae: 8.9920 - val_loss: 64.4343 - val_mae: 5.9857\n",
      "Epoch 3/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 136.2446 - mae: 8.9224 - val_loss: 70.9856 - val_mae: 6.0373\n",
      "Epoch 4/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 124.3577 - mae: 8.5320 - val_loss: 62.4581 - val_mae: 5.6644\n",
      "Epoch 5/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 130.9512 - mae: 8.7676 - val_loss: 68.2254 - val_mae: 5.9284\n",
      "Epoch 6/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 130.2110 - mae: 8.8207 - val_loss: 60.5007 - val_mae: 5.5967\n",
      "Epoch 7/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 134.8764 - mae: 8.8805 - val_loss: 61.1891 - val_mae: 5.6218\n",
      "Epoch 8/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 133.3353 - mae: 8.8841 - val_loss: 69.0604 - val_mae: 6.0474\n",
      "Epoch 9/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 134.5361 - mae: 8.9119 - val_loss: 63.7336 - val_mae: 5.7979\n",
      "Epoch 10/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 133.8186 - mae: 8.9027 - val_loss: 65.9743 - val_mae: 5.8546\n",
      "Epoch 11/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 130.5109 - mae: 8.7601 - val_loss: 58.9980 - val_mae: 5.5834\n",
      "Epoch 12/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 133.8236 - mae: 8.7997 - val_loss: 72.0263 - val_mae: 6.5045\n",
      "Epoch 13/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 139.7609 - mae: 9.0845 - val_loss: 60.9948 - val_mae: 5.5772\n",
      "Epoch 14/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 128.8895 - mae: 8.6088 - val_loss: 62.2828 - val_mae: 5.8566\n",
      "Epoch 15/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 134.9856 - mae: 8.7179 - val_loss: 62.0961 - val_mae: 5.8730\n",
      "Epoch 16/200\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 128.7023 - mae: 8.7077 - val_loss: 60.0275 - val_mae: 5.6038\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 57.8259 - mae: 5.5470\n",
      "Validation MAE: 5.58\n"
     ]
    }
   ],
   "source": [
    "# 콜백 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# 모델 학습\n",
    "history = dbp_model.fit(\n",
    "    X_train_combined, DBP_Y_train_combined,\n",
    "    epochs=200,\n",
    "    batch_size=50,\n",
    "    validation_data=(X_test_combined, DBP_Y_test_combined),\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# 검증 데이터로 모델 평가\n",
    "val_loss, val_mae = dbp_model.evaluate(X_test_combined, DBP_Y_test_combined)\n",
    "print(f\"Validation MAE: {val_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 138.0439 - mae: 9.0050 - val_loss: 97.3511 - val_mae: 7.3367\n",
      "Epoch 2/150\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 135.1364 - mae: 8.8817 - val_loss: 61.9033 - val_mae: 5.7668\n",
      "Epoch 3/150\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 145.3266 - mae: 9.1528 - val_loss: 57.8832 - val_mae: 5.5126\n",
      "Epoch 4/150\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 133.5153 - mae: 8.8776 - val_loss: 67.0152 - val_mae: 5.9306\n",
      "Epoch 5/150\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 134.6318 - mae: 8.8682 - val_loss: 62.7252 - val_mae: 5.6580\n",
      "Epoch 6/150\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 128.1153 - mae: 8.6456 - val_loss: 65.9646 - val_mae: 5.7953\n",
      "Epoch 7/150\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 126.0608 - mae: 8.6189 - val_loss: 61.7360 - val_mae: 5.6756\n",
      "Epoch 8/150\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 125.8873 - mae: 8.5433 - val_loss: 60.7398 - val_mae: 5.6735\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 58.6622 - mae: 5.5468\n",
      "Validation MAE: 5.51\n"
     ]
    }
   ],
   "source": [
    "# 콜백 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# 모델 학습\n",
    "history = dbp_model.fit(\n",
    "    X_train_combined, DBP_Y_train_combined,\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_combined, DBP_Y_test_combined),\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# 검증 데이터로 모델 평가\n",
    "val_loss, val_mae = dbp_model.evaluate(X_test_combined, DBP_Y_test_combined)\n",
    "print(f\"Validation MAE: {val_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
